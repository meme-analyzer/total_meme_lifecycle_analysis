import os
import pandas as pd
import numpy as np
import re
from datetime import datetime
import sys
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer

# âœ… ê²½ë¡œ ì„¤ì • (ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ config ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ sys.path ì¶”ê°€)
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import RAW_DATA_DIR, PROCESSED_DATA_DIR

class SeleniumTwitterPreprocessor:
    def __init__(self):
        # âœ… ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • ë° ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë”©
        self.raw_data_dir = RAW_DATA_DIR
        self.processed_data_dir = PROCESSED_DATA_DIR
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

    def load_twitter_data(self, filename):
        # âœ… ì›ì‹œ íŠ¸ìœ„í„° ë°ì´í„° CSV ë¡œë“œ
        filepath = os.path.join(self.raw_data_dir, filename)
        df = pd.read_csv(filepath)
        print(f"ğŸ“… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ ê²Œì‹œë¬¼")
        return df

    def preprocess(self, df):
        print("\nğŸ§¹=== íŠ¸ìœ„í„° ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

        # âœ… ë‚ ì§œ, ì‹œê°„, ìš”ì¼ ì¶”ì¶œ
        df['created_at'] = pd.to_datetime(df['created_at'])
        df['date'] = df['created_at'].dt.strftime('%Y-%m-%d')  # ë¬¸ìì—´ë¡œ ë³€í™˜í•´ ì €ì¥
        df['hour'] = df['created_at'].dt.hour
        df['day_of_week'] = df['created_at'].dt.dayofweek
        df['day_abbr'] = df['created_at'].dt.strftime('%a').str.upper()  # âœ… MON, TUE í˜•ì‹ ìš”ì¼ ì¶”ê°€

        # âœ… ê²°ì¸¡ê°’ ì²˜ë¦¬
        df['text'] = df['text'].fillna('')
        df['author'] = df['author'].fillna('[deleted]')

        # âœ… ìˆ«ìí˜• ì»¬ëŸ¼ ì²˜ë¦¬: ì½¤ë§ˆ ì œê±° í›„ ìˆ«ìë¡œ ë³€í™˜
        df['likes'] = pd.to_numeric(df['likes'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
        df['retweets'] = pd.to_numeric(df['retweets'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
        df['views'] = pd.to_numeric(df['views'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)

        # âœ… í…ìŠ¤íŠ¸ ì •ì œ
        df['text_clean'] = df['text'].apply(self.clean_text)

        # âœ… ì°¸ì—¬ ì ìˆ˜ ê³„ì‚° (ì¢‹ì•„ìš” + 2*ë¦¬íŠ¸ìœ— + 0.1*ì¡°íšŒìˆ˜)
        df['engagement_score'] = df['likes'] + df['retweets'] * 2 + df['views'] * 0.1

        # âœ… ìµœì‹ ìˆœ ì •ë ¬
        df = df.sort_values(by='created_at', ascending=False).reset_index(drop=True)

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ê²Œì‹œë¬¼")
        return df

    def clean_text(self, text):
        # âœ… URL ì œê±° ë° ê³µë°± ì •ë¦¬
        if pd.isna(text) or text == '':
            return ''
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def analyze_temporal_patterns(self, df):
        # âœ… ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ì¼ìë³„, ì‹œê°„ëŒ€ë³„, ìš”ì¼ë³„)
        print("\nâ±ï¸=== ì‹œê°„ íŒ¨í„´ ë¶„ì„ ===")
        daily_posts = df.groupby('date').size()
        hourly_dist = df['hour'].value_counts().sort_index()
        day_dist = df['day_of_week'].value_counts().sort_index()
        days = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']

        print(f"ğŸ—–ï¸ ë°ì´í„° ê¸°ê°„: {daily_posts.index.min()} ~ {daily_posts.index.max()}")
        print(f"ğŸ“ˆ ì¼ í‰ê·  ê²Œì‹œë¬¼ ìˆ˜: {daily_posts.mean():.2f}")
        print(f"â° ê°€ì¥ í™œë°œí•œ ì‹œê°„ëŒ€: {hourly_dist.idxmax()}ì‹œ")
        print(f"ğŸ—“ï¸ ê°€ì¥ í™œë°œí•œ ìš”ì¼: {days[day_dist.idxmax()]}ìš”ì¼")

        return {
            'daily_posts': daily_posts,
            'hourly_dist': hourly_dist,
            'day_dist': day_dist
        }

    def perform_clustering(self, df, n_clusters=5):
        # âœ… ë¬¸ì¥ ì„ë² ë”© í›„ PCA ì¶•ì†Œ + KMeans í´ëŸ¬ìŠ¤í„°ë§
        print("\nğŸ”—=== í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ ===")
        embeddings = self.embedder.encode(df['text_clean'].tolist(), show_progress_bar=True)
        pca = PCA(n_components=2)
        reduced = pca.fit_transform(embeddings)
        df['x'] = reduced[:, 0]
        df['y'] = reduced[:, 1]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(reduced)
        df['cluster'] = kmeans.labels_
        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ (êµ°ì§‘ ìˆ˜: {n_clusters})")
        return df

    def estimate_last_seen(self, df):
        # âœ… ë™ì¼í•œ í…ìŠ¤íŠ¸ ê¸°ì¤€ ë§ˆì§€ë§‰ ë“±ì¥ ì‹œì  ì¶”ì •
        print("\nğŸ”=== ìƒì¡´ ë¶„ì„ìš© ë§ˆì§€ë§‰ ë“±ì¥ ì‹œì  ì¶”ì • ===")
        grouped = df.groupby('text_clean')['created_at'].max().reset_index()
        grouped.columns = ['text_clean', 'last_seen_at']
        df = df.merge(grouped, on='text_clean', how='left')
        print("âœ… last_seen_at ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ")
        return df

    def save_processed_data(self, df, output_filename):
        # âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ë° ìš”ì•½ í†µê³„ ì €ì¥
        os.makedirs(self.processed_data_dir, exist_ok=True)
        output_path = os.path.join(self.processed_data_dir, output_filename)
        df.to_csv(output_path, index=False)
        print(f"ğŸ“‚ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")

        summary = {
            'total_posts': len(df),
            'date_range': f"{df['created_at'].min()} ~ {df['created_at'].max()}",
            'unique_authors': df['author'].nunique(),
            'avg_likes': df['likes'].mean(),
            'avg_retweets': df['retweets'].mean(),
            'avg_views': df['views'].mean()
        }

        summary_path = output_path.replace('.csv', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            for key, value in summary.items():
                f.write(f"{key}: {value}\n")

        return df
